{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYBgCIMVIPHy"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import time\n",
        "import random\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fiyatlarliste=[]\n",
        "metrekareliste=[]\n",
        "yasliste=[]\n",
        "konumliste=[]\n",
        "odaliste=[]\n",
        "#For döngüsünde html üzerinde gezinirken bulguları bu listelere aktaracağız."
      ],
      "metadata": {
        "id": "J35GVPSMJIbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2,487):\n",
        "    adres = f\"https://www.hepsiemlak.com/istanbul-kiralik?page={i}\"\n",
        "    #Sayfa uzantıları 2. sayfadan sonra belli bir doğrultuda ilerlediği için range() ile kolayca çalışabilmemizi sağladı.\n",
        "    baslik = {\"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"}\n",
        "    sayfa = requests.get(adres, headers=baslik)\n",
        "    soup = bs(sayfa.content, features=\"lxml\")\n",
        "    fiyatlar = soup.find_all(\"span\",{\"class\" : \"list-view-price\"})\n",
        "    metrekare = soup.find_all(\"span\",{\"class\" : \"celly squareMeter list-view-size\"})\n",
        "    yas = soup.find_all(\"span\",{\"class\" : \"celly buildingAge\"})\n",
        "    konum = soup.find_all(\"div\",{\"class\" : \"list-view-location\"})\n",
        "    oda = soup.find_all(\"span\",{\"class\" : \"celly houseRoomCount\"})\n",
        "    time.sleep(random.randint(3,7))\n",
        "    #Veriyi çektiğimiz site tarafından bot olarak algılanmamak için random sleep ile insan taklidi yapıyoruz.\n",
        "    for i in range(len(fiyatlar)):\n",
        "        fiyatlarliste.append(int(fiyatlar[i].text.replace(\".\",\"\").replace(\"TL\",\"\").strip()))\n",
        "        metrekareliste.append(int(metrekare[i].text[:-2].replace(\".\",\"\").strip()))\n",
        "        yasliste.append(int(yas[i].text.replace(\" Yaşında\",\"\").replace(\"Sıfır Bina\",\"1\").strip()))\n",
        "        konumliste.append(konum[i].text.replace(\" \",\"\").replace(\"\\n\",\"\").replace(\"Mahallesi\",\" Mahallesi\").strip())\n",
        "        odaliste.append(oda[i].text.strip())\n",
        "        #Strip() ile metin  başı ve sonundaki boşlukları temizledik. Veriyi istediğimiz şekilde düzenledik.\n"
      ],
      "metadata": {
        "id": "OjM4BZVPJLo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(list(zip(fiyatlarliste,metrekareliste,yasliste,konumliste,odaliste)),columns=[\"Fiyatlar\",\"Metrekare\",\"Yas\",\"Konum\",\"Oda\"])\n",
        "data.to_csv('verilerist4.csv',encoding=\"utf-8\", index=False)\n",
        "#Listelerimizi birleştirip dataframe haline getirdik ve dışarı aktardık."
      ],
      "metadata": {
        "id": "1dZd7kPeJQoC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
